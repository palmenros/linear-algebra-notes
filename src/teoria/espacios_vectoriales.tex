% !TeX root = ../algebra_lineal.tex

\documentclass[../algebra_lineal.tex]{subfiles}

\begin{document}

\section{Notación}

¿Explicar aquí notación del tal que (aunque estoy poniendo tal que todo el rato con palabras)? ¿Explicar aquí la notación del subset (aunque estoy intentando utilizar todo el rato subseteq)? ¿Explicar aquí que los superíndices no son potencias? ¿Introducción a la importancia del álgebra lineal?

\section{Cuerpos}

Para formalizar el concepto de espacio vectorial, deberemos antes formalizar el conjunto de escalares, es decir, un conjunto ``de dimensión uno'', signifique lo que signifique eso, que podremos utilizar para multiplicar (``escalar'') vectores. Habitualmente usaremos $\R$ o $\Complex$, pero en esta sección vamos a intentar extraer las propiedades relevantes de estos dos conjuntos para definir una estructura algebraica abstracta, el \textit{cuerpo}, del que únicamente conoceremos sus propiedades.

Informalmente, las propiedades más relevantes son la existencia de las operaciones suma y multiplicación, que estas sean algebraicamente cerradas
(es decir, que al sumar o multiplicar dos elementos del cuerpo obtengamos otro elemento del cuerpo), que cumplan las propiedades asociativa, conmutativa
\footnote{En ocasiones se distinguen a los cuerpos que cumplen la propiedad conmutativa como \textit{cuerpos abelianos}, en este texto todos los cuerpos
 serán conmutativos} y distributiva, que existan elementos neutros (0 para la suma y 1 para la multiplicación), que exista el concepto de \textit{división} (es decir, que cada elemento tenga un inverso multiplicativo), el concepto de \textit{resta} (es decir, que cada elemento tenga un opuesto aditivo).

Analicemos ahora la definición formal.

\begin{definition}[Cuerpo]
    Decimos que $\K$ es un cuerpo si existen dos aplicaciones $+:\K \times \K \to \K$, $\cdot:\K \times \K \to \K$ llamadas respectivamente suma y multiplicación que satisfacen las siguientes propiedades:
    \begin{enumerate}
        \item Asociatividad de la suma y multiplicación: $a+(b+c)=(a+b)+c$, $a\cdot(b\cdot c)=(a\cdot b)\cdot c$
        \item Conmutatividad de la suma y multiplicación: $a+b=b+a$, $a\cdot b = b \cdot a$
        \item Distributividad de la multiplicación sobre la suma: $a \cdot (b+c) = (a \cdot b) + (a \cdot c)$
        \item Elemento neutro para la suma: $\exists 0 \in \K \st \forall a \in \K, \spc a+0=a$
        \item Elemento identidad para la multiplicación:  $\exists 1 \in \K \st \forall a \in \K, \spc a\cdot 1=a$
        \item \label{cuerpo_opuesto} Opuesto aditivo: $\forall a \in \K, \spc \exists {(-a)} \in \K \st a + (-a) = 0 $.
        \item \label{cuerpo_inverso} Inverso multiplicativo: $\forall a \neq 0 \in \K, \spc \exists {(a^{-1})} \in \K \st a \cdot (a^{-1}) = 1$
    \end{enumerate}    
\end{definition}

\begin{notation}
    En las propiedades \ref{cuerpo_opuesto} y \ref{cuerpo_inverso} de los opuestos e inversos, hemos realizado un pequeño abuso de notación por motivos de claridad. Lo que realmente indica la propiedad es que para cualquier $a \in \K$ existe otro elemento $b_a \in \K$ (indicamos con el subíndice que depende de $a$) tal que $a + b_a = 0$. Denotamos al elemento $b_a$ como $-a$. Análogamente hacemos lo propio con el inverso multiplicativo. 
\end{notation}

\begin{remark}
    Sea cual sea el cuerpo, contiene a todos los números naturales ya que si $n \in \N \implies n = 1 + 1 + \dots + 1 \in \K$ porque cada uno de los $1$ pertenece a $\K$, por lo que la suma pertenece también.
\end{remark}

Con esta definición de cuerpo hemos ampliado significativamente el rango de conjuntos que podemos usar. Sin embargo, surgen algunos casos patológicos, como por ejemplo $\Z_2 = \set{0, 1}$, el cuerpo de los enteros módulo 2 (es decir, se ejecuta la aritmética usualmente y finalmente colapsamos los pares al 0 y los impares al 1). Se deja como ejercicio al lector demostrar que efectivamente $\Z_2$ es un cuerpo (de hecho, $\Z_p$ es un cuerpo para todo $p$ primo, algo fuera del alcance de este texto). La patología de $\Z_2$ radica en que $1+1=0$. A menudo querremos dividir entre 2 en variadas demostraciones, por lo que impondremos un requisito especial a los cuerpos que utilizaremos. Para ello, debemos introducir la definición de \textit{característica}.

\begin{definition}
    Llamamos característica de $\K$ al mínimo número de veces que es necesario sumar el elemento identidad 1 en una suma para obtener el elemento neutro 0. Es decir, la característica es el entero positivo $n$ más pequeño tal que:
    \[
        \underbrace{1+\dots+1}_{n\, \, \mathrm{sumandos}} = 0
    \]  
\end{definition}

A partir de ahora, salvo que mencionemos expresamente lo contrario, siempre hablaremos de cuerpos $\K$ de característica distinta de 2 (es decir, $1+1 \ne 0$).

\section{Espacios vectoriales}

Al igual que hicimos con los cuerpos, vamos a definir los espacios vectoriales usando únicamente las propiedades que lo definen. Más adelante relacionaremos esta definición con el concepto intuitivo de espacio vectorial utilizando coordenadas.

Al contrario que hicimos con los cuerpos, para definir un espacio vectorial necesitaremos un cuerpo $\K$ de característica distinta de 2, que utilizaremos para multiplicar los vectores, por lo que el cuerpo aparecerá en la definición de espacio vectorial.

\begin{definition}[Espacio vectorial]
    \label{vector_space_definition}
    Un conjunto $V \neq \emptyset$ es un espacio vectorial sobre $\K$ (también diremos que es un $\K$-espacio vectorial) si existen las siguientes operaciones en $V$ (suma y multiplicación por escalar) 
    
    \begin{multicols}{2}
        \noindent
        \begin{align*} +: V \times V &\to V \\
                (\vx, \vy) &\mapsto \vx + \vy
        \end{align*}
        \begin{align*}
            \cdot: \K \times V &\to V \\
                (a, \vx) &\mapsto a \vx
        \end{align*}
    \end{multicols}

    y satisfacen las siguientes propiedades:

    \begin{enumerate}
        \item Conmutatividad de la suma: $\forall \vx, \vy \in V, \spc \vx + \vy = \vy + \vx$
        \item Asociatividad de la suma: $\forall \vx, \vy, \vz \in V, \spc \vx + (\vy + \vz) = (\vx + \vy) + \vz$
        \item \label{espacio_vectorial_neutro} Elemento neutro: $\exists \zv \in V \st \forall \vx \in V, \spc \vx + \zv = \vx$
        \item \label{espacio_vectorial_opuesto} Elemento opuesto: $\forall \vx \in V, \spc \exists {({-x})} \in V \st  \vx + (-\vx) = \zv$
        \item Distributiva vectorial: $\forall a \in \K, \spc \forall \vx, \vy \in V, \spc a(\vx + \vy) = a\vx + a \vy$
        \item Distributiva escalar: $\forall a, b\in \K, \spc \forall \vx \in V, \spc (a+b)\vx = a\vx + b\vx$ 
        \item Asociativa: $\forall a, b \in \K, \spc \forall \vx \in V, \spc a \cdot (b \cdot \vx) = (a\cdot b) \cdot \vx $
        \item \label{espacio_vectorial_unidad} Elemento unidad: $\forall \vx \in V, \spc 1 \cdot \vx = \vx$ 
    \end{enumerate}

\end{definition}

\begin{remark}
    Es importante distinguir el elemento neutro $0 \in \K$ del cuerpo escalar, del elemento neutro $\zv \in V$ del espacio vectorial, ya que uno es un vector y otro un escalar. Sin embargo, el elemento identidad $1 \in \K$ de la propiedad \ref{espacio_vectorial_unidad} sí que es un escalar (se suman vectores, y se multiplican vectores con escalares).  
\end{remark}

\begin{notation}
    Si $V$ es un \kvspace, llamaremos a los elementos de V \textit{vectores} y a los elementos de $\K$ escalares.
\end{notation}

A partir de ahora, si no indicamos otra cosa, $V$ será un \kvspace, siendo $\K$ un cuerpo de característica distinta de 2.

Es posible que esta definición abstracta de espacio vectorial parezca al lector algo árida, sin embargo es una herramienta muy potente que nos permitirá desarrollar una teoría que se puede aplicar a cualquier conjunto que satisfaga la definición de espacio vectorial, no únicamente a $\K^n$. Ejemplos de espacios vectoriales muy útiles en las matemáticas son los polinomios $\K[x]$ \footnote{Denotaremos como $\K[x]$ el conjunto de polinomios de cualquier grado sobre la variable $x$, es decir, polinomios de la forma, $a_n x^n + \dots + a_1 x + a_0$ con $a_i \in \K \spc \forall i \in {1, \dots, n}$}, las funciones continuas, las soluciones de ecuaciones diferenciales lineales, etc. Si desarrollamos todo desde las propiedades de espacio vectorial, podremos aplicar toda la teoría que desarrollemos a todos esos espacios vectoriales y muchos otros, sin tener que distinguir casos durante el desarrollo de la teoría.

A pesar de la potencia de esta aproximación a la definición de espacio vectorial, debemos demostrar incluso las cosas más nimias desde las propiedades. Comenzamos demostrando algunas propiedades inmediatas.

\begin{proposition}
    Sea $V$ un \kvspace. Entonces, las siguientes propiedades son ciertas:
    \begin{enumerate}
        \item $a \cdot \zv = \zv \spc \forall a \in \K$
        \item $a(-\vx) = -a\cdot \vx \spc \forall a \in \K$
        \item $0 \cdot \vx = \zv \spc \forall \vx \in V$
        \item \label{propiedad_cuatro_espacios_vectoriales} $(-a)\cdot \vx = -a\vx$
        \item Si $\vx \ne 0$, entonces $a \cdot \vx = \zv \implies a = 0$
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}[wide, labelwidth=0pt, labelindent=0pt]
        \item Primero observamos que $a\cdot \zv = a(\zv + \zv) = a \cdot \zv + a \cdot \zv $. Restando a ambos lados $a\cdot \zv$ obtenemos que 
            \[\zv = a\cdot \zv + (-a\cdot \zv) = (a \cdot \zv + a \cdot \zv) + (-a \cdot \zv) =  a \cdot \zv + \underbrace{(a \cdot \zv + (-a \cdot \zv))}_{\zv} = a \cdot \zv \]
        \item Primero nos percatamos de que $\zv = a \cdot \zv = a(\vx + (-\vx)) = a\vx +a\cdot(-\vx)$. Como $\zv$ es igual a dicha expresión, podemos sustituir la expresión de la derecha en cualquier lugar en el que aparezca $\zv$ por lo que
         \[ -a\vx = -a\vx + \zv = -a\vx + (a\vx + a(-\vx)) = \underbrace{(-a\vx + a\vx)}_{\zv} + a(-\vx) = a(-\vx) \] 
        \item En primer lugar vemos que $0 \cdot \vx = (0+0)\vx = 0 \cdot \vx + 0 \cdot \vx$. De nuevo, podremos sustituir eso en la siguiente ecuación
        \[\zv  = 0 \cdot \vx +(-0\cdot \vx) = (0\cdot \vx + 0\cdot \vx) + (-0\cdot \vx) =  0\cdot \vx + \underbrace{(0\cdot \vx + (-0\cdot \vx))}_{\zv} = 0\cdot\vx\]
        \item Observamos que $\zv = 0 \cdot \vx = (a + (-a))\vx = a\vx + (-a)\vx$. Sumar $\zv$ es equivalente a no hacer nada, por lo que podremos sumar la expresión de la derecha sin cambiar ningún vector
        \[-a\vx = -a\vx + (a\vx + (-a)\vx) = \underbrace{(-a\vx + a\vx)}_{\zv} + (-a)\vx = (-a)\vx \]
        \item Lo demostraremos por reducción al absurdo. Supongamos que $a \neq 0$, entonces $\frac{1}{a} \in \K$. Por tanto
        \[\vx = \underbrace{\left(\frac{a}{a} \right)}_{1} \vx = \frac{1}{a} (a \cdot \vx) = \frac{1}{a} \cdot \zv = \zv\]
        Hemos llegado a que $\vx = 0$, lo que es una contradicción.
    \end{enumerate}
\end{proof}

\begin{notation}
    Utilizaremos la notación usual en la que restar significa sumar el opuesto, es decir, $\vx - \vy = \vx + (-\vy)$
\end{notation}

\begin{remark}
    Si particularizamos la propiedad \ref{propiedad_cuatro_espacios_vectoriales} para $a=1$, obtenemos que $-\vx = (-1)\vx$, por lo que para obtener el vector opuesto basta con multiplicar por el opuesto del escalar $1$ (es decir, por $-1$).
\end{remark}

\begin{example}
    Veamos varios ejemplos de espacios vectoriales:
    \begin{itemize}
        \item Un cuerpo es un espacio vectorial sobre él mismo: $\K$ es un \kvspace.
        \item $\R$ es un $\Q$-espacio vectorial definiendo la suma y el producto usualmente
        
        \begin{multicols}{2}            
            \noindent\begin{align*}
                +: \R \times \R &\to \R \\
                    (a, b) &\mapsto a+b
            \end{align*}
            \begin{align*}
                \cdot: \Q \times \R &\to \R \\
                    \parens{\frac{p}{q}, a} &\mapsto \frac{p \cdot a}{q}
            \end{align*}
        \end{multicols}

        \item Si $n \in \N$, $\K^n$ es un espacio vectorial definiendo la suma de tuplas como la suma elemento a elemento y la multiplicación de una tupla por un escalar como la multiplicación de cada elemento de la tupla por dicho escalar, es decir,
        \begin{align*} +: \K^n \times \K^n &\to \K^n \\
                \parens{\parens{\dlst{a}{n}}, \parens{\dlst{b}{n}}} &\mapsto \parens{a_1+b_1, a_2+b_2, \dots, a_n+b_n}
        \end{align*}
        \begin{align*}
            \cdot: \K \times \K^n &\to \K^n \\
                (\lambda, \parens{\dlst{a}{n}}) &\mapsto \parens{\lambda \cdot a_1, \lambda \cdot a_2, \dots, \lambda \cdot  a_n}
        \end{align*}

    \end{itemize}
\end{example}

\section{Subespacios vectoriales}

En matemáticas es muy común querer trabajar con subconjuntos de un conjunto más grande. Habrá ocasiones en las que nos convenga trabajar por ejemplo con rectas o planos en vez de con el espacio vectorial total. Sin embargo, queremos que esos subconjuntos sigan teniendo la estructura de espacio vectorial, para poder aplicar a ellos también toda la teoría que estamos desarrollando. Nace de ahí el concepto de \textit{subespacio vectorial}, que informalmente es un subconjunto que con las \textbf{operaciones inducidas} del espacio vectorial que le contiene, sigue siendo a su vez un espacio vectorial. Es muy importante que las operaciones sigan siendo las mismas, es decir, la suma es la misma que en el espacio vectorial \textit{ambiente} (el espacio vectorial que lo contiene), para después poder juntar y usar varios subespacios vectoriales a la vez.

Para que las operaciones inducidas puedan ser efectivamente las operaciones de un espacio vectorial, estas deben ser cerradas. Es decir, la suma de vectores en el subespacio vectorial debe de seguir estando en el subespacio vectorial, y la multiplicación de un vector del subespacio por un escalar debe seguir estando en el subespacio. Esto es una condición necesaria para que un subconjunto sea subespacio vectorial, y es tan importante que hasta merece nombre.

\begin{definition}
    Un subconjunto no vacío $L\subseteq V$ es parte estable si verifica:
    \begin{enumerate}
        \item $\forall \vx, \vy \in L \implies \vx + \vy \in L$
        \item $\forall \vx \in L, \spc \forall a \in \K \implies a\cdot \vx \in L$
    \end{enumerate}
\end{definition}

Estamos ahora en condiciones de definir qué es un subespacio vectorial (sabiendo que ser parte estable es condición necesaria).

\begin{definition}[Subespacio vectorial]
    Una parte estable $L$ de $V$ es un subespacio vectorial de $V$ si $L$ con las operaciones inducidas por las de $V$ en $l$ es un \kvspace.
\end{definition}

\begin{remark}
    Las operaciones inducidas por $V$ en $L$ están bien definidas únicamente si $L$ es parte estable, y son las siguientes:
    \begin{multicols}{2}
        \noindent
        \begin{align*} +: L \times L &\to L \\
                (\vx, \vy) &\mapsto \vx + \vy \in L
        \end{align*}
        \noindent
        \begin{align*}
            \cdot: \K \times L &\to L \\
                (a, \vx) &\mapsto a \vx \in L
        \end{align*}
    \end{multicols} 
\end{remark}

De hecho, ser parte estable no es solamente condición necesaria, sino que también es condición suficiente.

\begin{proposition}
    $L$ es parte estable si y solamente si $L$ es un subespacio vectorial
\end{proposition}

\begin{proof}
    Solamente debemos demostrar la suficiencia. Sea $L \subseteq V$ parte estable. Únicamente debemos demostrar las propiedades \ref{espacio_vectorial_neutro} y \ref{espacio_vectorial_opuesto} de la definición \ref{vector_space_definition} de espacio vectorial para ver que $L$ es un \kvspace, ya que el resto de propiedades se cumplen en $V$, por lo que particularmente se cumplen en $L$.
    
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item Sea $\vx \in L$, entonces $-\vx = \underbrace{(-1)}_{\in \K} \cdot \underbrace{\vx}_{\in V} \in L$. Pertenece a $L$ por ser $L$ parte estable.
        \setcounter{enumi}{2}
        \item Sea $\vx \in L$, entonces $\underbrace{\vx}_{\in L} + \underbrace{(-\vx)}_{\in L} = \zv \in L$. De nuevo pertenece a $L$ por ser $L$ parte estable.
    \end{enumerate}
    $ $
\end{proof}

Las operaciones de las que disponen los espacios son la suma de vectores y multiplicación por escalares, que usualmente combinaremos para dar lugar a las \textit{combinaciones lineales}

\begin{definition}[Combinación lineal]
    \label{definicion_combinacion_lineal}
    Sean $p \in \N, \spc \dlst{\vx}{p} \in V, \spc \ulst{a}{p} \in \K$. Si $\vx = \lincomb{a}{x}{p}$, diremos que $\vx$ es combinación linear de $\dlst{x}{p}$ con coeficientes $\ulst{a}{p}$.

\end{definition}

\begin{remark}
    En la definición \ref{definicion_combinacion_lineal} los superíndices no indican exponentes ni potencias, sino que se usan para distinguir distintos escalares. Como regla general, indexaremos los escalares por superíndices y los vectores con subíndices por motivos de claridad.
\end{remark}

Las combinaciones lineales son especialmente importantes porque juntan de manera concisa las sumas y multiplicaciones por escalares. Permiten usualmente expresar las propiedades que normalmente se deberían enunciar por separado en una sola propiedad, utilizando combinaciones lineales en vez de sumas y multiplicaciones. 

El primer ejemplo de esto es que los subespacios vectoriales son los conjuntos que continenen a las combinaciones lineales de sus vectores.


\begin{proposition}
    $\emptyset \ne L \subseteq V$ es subespacio vectorial $\iff \forall p \in N, \spc \forall \dlst{x}{p} \in L, \spc \forall \ulst{a}{p} \in \K, \spc \lincomb{a}{x}{p} \in L$.
\end{proposition}

\begin{proof}
    Veamos las dos implicaciones por separado.
    \begin{enumerate}
        \item[\protect\fbox{$\Rightarrow$}] Supongamos que $L$ es subespacio vectorial y sea $p \in \N$ \\
        Sean $\dlst{x}{p} \in L, \spc \ulst{a}{p} \in \K, \spc \lincomb{a}{x}{p} \in L$. Entonces, $a^1 \vx_1, a^2\vx_2, \dots, a^p\vx_p \in L$ por ser $L$ subespacio vectorial, y de nuevo, por ser $L$ subespacio vectorial se da que $\lincomb{a}{x}{p} \in L$. 
        \item[\protect\fbox{$\Leftarrow$}] Si $L$ contiene las combinaciones lineales, particularmente contiene cada suma y cada multiplicación por escalar, así que es parte estable y por tanto espacio vectorial.
    \end{enumerate} 
\end{proof}

\section{Bases de espacios vectoriales}

No todos los subconjuntos son espacios vectoriales, pero nos puede interesar encontrar un subespacio vectorial que contenga a determinado conjunto sin que este sea el total. Después de introducir las combinaciones lineales en la sección anterior, no es descabellada la idea de definir el conjunto de todas las combinaciones lineales de otro conjunto.

\begin{definition}
    Sea $\emptyset \neq H \subseteq V$ un subconjunto de $V$ que puede ser o no ser un espacio vectorial. Definimos $\lspan{H}$ como el conjunto de todas las combinaciones lineales de $H$, es decir,
    \[
        \lspan{H} = \set{\lincomb{a}{x}{p} : p \in \N, \spc \dlst{x}{p} \in L, \spc \ulst{a}{p} \in \K}
    \]
\end{definition}

En las siguientes proposiciones veremos que $\lspan{H}$ efectivamente contiene a $H$ y que efectivamente es un subespacio vectorial.

\begin{proposition}
    $H \subseteq \lspan{H}$
\end{proposition}

\begin{proof}
    Si $\vx \in H \implies \underbrace{1}_{\in \K} \cdot \underbrace{\vx}_{\in H} = \vx \in \lspan{H} \implies H \subseteq \lspan{H}$
    $ $.
\end{proof}

\begin{proposition}
    $\lspan{H}$ es un subespacio vectorial de $V$.
\end{proposition}

\begin{proof}
    Veamos que $\lspan{H}$ es parte estable.

    Sean $\vx, \vy \in \lspan{H} \Rightarrow \exists p, q \in \N, \spc \dlst{\vx}{p}, \dlst{\vy}{q} \in H, \, \, \ulst{a}{p}, \ulst{b}{q} \in \K $
    tal que 
    \begin{align*}
        \vx &= \lincomb{a}{x}{p} \\
        \vy &= \lincomb{b}{y}{q}
    \end{align*}

    Entonces podemos representar $\vx + \vy$ como
    
    \[
        \vx + \vy = a^1\underbrace{\vx_1}_{\in H}+a^2\underbrace{\vx_2}_{\in H}+\dots+a^p\underbrace{\vx_p}_{\in H} + b^1\underbrace{\vy_1}_{\in H}+b^2\underbrace{\vy_2}_{\in H}+\dots+b^p\underbrace{\vy_p}_{\in H}
    \]

    Por tanto $\vx + \vy$ es una combinación lineal de vectores de $H$, por lo que $\vx + \vy \in \lspan{H}$. Analizemos ahora el producto por escalar. Sea $a \in \K, \vx \in \lspan{H} \implies \exists p \in \N, \space \exists \dlst{\vx}{p} \in H$ y también $\exists \ulst{a}{p} \in \K \st \vx = \lincomb{a}{x}{p}$.  Multiplicamos a ambos lados por $a$:
    \[a\cdot \vx = a \left( \lincomb{a}{x}{p} \right) = \underbrace{(a \cdot a^1)}_{\in \K}\underbrace{\vx_1}_{\in H} + \underbrace{(a \cdot a^2)}_{\in \K}\underbrace{\vx_2}_{\in H} + \dots + \underbrace{(a \cdot a^p)}_{\in \K}\underbrace{\vx_p}_{\in H}\]

    Por tanto $a\vx$ es una combinación lineal finita de vectores de $H$, por lo que $a\vx \in \lspan{H}$.

    Concluimos por tanto que $\lspan{H}$ es un subespacio vectorial.
\end{proof}

De hecho, $\lspan{H}$ no es solamente un subespacio vectorial que contiene a $H$, sino que es el subespacio vectorial más pequeño que contiene a $H$. 

\begin{proposition}
    Si $L$ es un subespacio vectorial de $V$ tal que $H \subseteq L \implies \lspan{H} \subseteq L$, es decir, $\lspan{H}$ es el subespacio más pequeño de los subespacios vectoriales que contienen a $H$.¡
\end{proposition}

\begin{proof}
Sea $\vx \in \lspan{H} \Rightarrow \exists p \in \N, \spc \dlst{x}{p} \in H, \spc \ulst{a}{p} \in \K$ tal que 

\[ \vx = a^1 \underbrace{\vx_1}_{\in H \subseteq L} +  a^2\underbrace{\vx_2}_{\in H \subseteq L} + \dots +  a^p \underbrace{\vx_p}_{\in H \subseteq L} \]

Como cada $\vx_i \in H \subseteq L$, entonces cada $\vx_i \in L$, por lo que $x$ es combinación lineal de vectores de $L$ y por ser $L$ un espacio vectorial, $\vx \in L$. Como esto se da para cada $\vx \in \lspan{H}$, $\lspan{H} \subseteq L$.
\end{proof}

\subsection{Sistemas generadores y linealmente independientes}

Hasta ahora hemos utilizado $\lspan{H}$ para encontrar un subespacio vectorial conociendo $H$. Sin embargo, podemos utilizarlo a la inversa. Dado un espacio vectorial $V$, si conseguimos expresarlo como $V=\lspan{H}$ para algún $H$ pequeño (por ejemplo, finito), sabemos que podemos expresar todos los vectores de $V$ como combinaciones lineales de vectores de $H$, lo cuál simplifica muchas cuentas.

Introduzcamos algunas definiciones basadas en esta idea.

\begin{definition}[Subespacio generado]
    Diremos que $\lspan{H}$ es el subespacio vectorial generado por $H$.
\end{definition}

\begin{definition}[Sistema de generadores]
    Diremos que $H$ es un sistema de generadores de $V$ si $\lspan{H} = V$.
\end{definition}

\begin{definition}[Finitamente generado]
    Diremos que $V$ es finitamente generado o de dimensión finita si existe un sistema de generadores de $V$ formado por un número finitio de elementos, es decir, si
    \[
        \exists p \in N, \spc \exists \dlst{x}{p} \in V \st \lspan{\set{\vlst{x}{p}}} = V
    \]  
\end{definition}

Durante este texto vamos a tratar únicamente con espacios vectoriales finitamente generados, no trataremos espacios vectoriales de dimensión infinita.

Ya hemos comentado anteriormente que cuanto más \textit{pequeño} sea $H$, más convenientes resultarán los cálculos, ya que podremos descomponer cualquier vector en una combinación lineal de menores vectores. Si uno de los vectores de $H$ fuese combinación lineal de los otros vectores, entonces no aportaría nada a $\lspan{H}$, podríamos eliminarlo y $\lspan{H'}$ seguiría manteniendose igual, y habríamos conseguido eliminar un vector de $H$. 

De este concepto nace la siguiente definición:

\begin{definition}[Familia linealmente dependiente]
    Si $p \ge 2$ y $\vlst{x}{p} \in V$. Diremos que la familia $\set{\vlst{x}{p}}$ es linealmente dependiente si uno de ellos es combinación lineal de los otros. En el caso $p=1$, un único vector es linealmente dependiente solo si es el vector nulo, en caso contrario, es linealmente independiente. Alternativamente, la familia $\set{\vlst{x}{p}}$ es linealmente dependiente si
    \[\exists i \in \set{1, 2, \dots, p} \st \vx_i \in \lspan{\set{\vlst{x}{i-1}, \vx_{i+1}, \dots, \vx_p}}
    \]
\end{definition}

Muchas veces nos resultará más útil hablar de \textit{familias linealmente independientes}.

\begin{definition}[Familia linealmente independiente]
    La familia $\set{\vlst{x}{p}}$ es linealmente independiente si no es linealmente dependiente. Alternativamente, $\set{\vlst{x}{p}}$ es linealmente independiente si
    \[\forall i \in \set{1, 2, \dots, p}, \spc \vx_i \notin \lspan{\set{\vlst{x}{i-1}, \vx_{i+1}, \dots, \vx_p}}
    \]
\end{definition}

Existe otra caracterización muy importante de la dependencia lineal, que nos permitirá más adelante demostrar resultados importantes.

\begin{proposition}
   Sean $\set{\vlst{x}{p}} \in V$. Entonces  $\set{\vlst{x}{p}}$ son linealmente dependientes si y solo si $\exists \slst{a}{p} \in \K$ no todos nulos tal que $\lincomb{a}{x}{p} = \zv$

\end{proposition}

\begin{proof}

    Analicemos las dos implicaciones por separado.
    \begin{enumerate}
        \item[\protect\fbox{$\Rightarrow$}] Supongamos $\set{\vlst{x}{p}}$ linealmente dependientes, es decir 
        \[\exists i \in \set{1, 2, \dots, p} \st \vx_i \in \lspan{\set{\vlst{x}{i-1}, \vx_{i+1}, \dots, \vx_p}}\]
        Por tanto, $\exists \slst{a}{i-1}, a^{i+1}, a^p \in \K$ tales que 
        \[
            \vx_i = \lincomb{a}{x}{i-1}+a^{i+1}\vx_{i+1}+\dots+a^p \vx_p  
        \]
        Restando $\vx_i$ a ambos lados de la ecuación llegamos a que
        \[
            \vx_i - \vx_i = \zv  = \lincomb{a}{x}{i-1}+\underbrace{(-1)}_{\ne 0}\vx_i +a^{i+1}\vx_{i+1}+\dots+a^p \vx_p
        \]
        \item[\protect\fbox{$\Leftarrow$}] Supongamos que $\exists \slst{a}{p} \in \K$ no todos nulos tal que $\lincomb{a}{x}{p} = 0$. Sea $i\in \set{1, \dots, p} \st a^i \ne 0 \Rightarrow \frac{1}{a} \in \K$. Por tanto,
        \begin{align*}
            \zv &= \frac{1}{a^i} \cdot \zv = \frac{1}{a^i}\parens{ \lincomb{a}{x}{i-1} + a^i\vx_i + a^{i+1}\vx_{i+1} + \dots a^p\vx_p} \\
                &= \parens{\frac{a^1}{a^i}}\vx_1 + \parens{\frac{a^2}{a^i}}\vx_2 + \dots + \parens{\frac{a^{i-1}}{a^i}}\vx_{i-1} + \underbrace{\parens{\frac{a^{i}}{a^i}}}_{1}\vx_{i} + \parens{\frac{a^{i+1}}{a^i}}\vx_{i+1} + \dots + \parens{\frac{a^{p}}{a^i}}\vx_{p}
        \end{align*}
        El coeficiente de $\vx_{i-1}$ es $1$, así que basta pasar el resto de términos que le acompañan restando para despejar $\vx_{i-1}$, obteniendo 
        \[
           \vx_{i} = \underbrace{\parens{\frac{-a^1}{a^i}}}_{\in \K}\vx_1 + \underbrace{\parens{\frac{-a^2}{a^i}}}_{\in \K}\vx_2 + \dots + \underbrace{\parens{\frac{-a^{i-1}}{a^i}}}_{\in \K}\vx_{i-1} + \underbrace{\parens{\frac{-a^{i+1}}{a^i}}}_{\in \K}\vx_{i+1} + \dots + \underbrace{\parens{\frac{-a^{p}}{a^i}}}_{\in \K}\vx_{p}
        \]

        Por tanto, $\vx_i$ es combinación lineal del resto de vectores.
    \end{enumerate}
\end{proof}

Negando los dos lados del si y solo si en la proposición anterior, conseguimos el siguiente corolario.

\begin{corollary}
    \label{linear_independence_corollary}
    Sean $\set{\vlst{x}{p}} \subseteq V$. Entonces $\set{\vlst{x}{p}}$ son linealmente independientes $\iff$ si  $\slst{a}{p} \in \K \st \lincomb{a}{x}{p} = \zv$ entonces $a^i = 0 \spc \forall i \in  \set{1, \dots, p} $. 
\end{corollary}

Una propiedad muy importante de las familias linealmente independientes, es que cualquier vector que se pueda expresar como combinación lineal de ellas se hace de manera única, es decir, no existen dos combinaciones lineales distintas de elementos de la familia. La demostración se apoya en la caracterízación que acabamos de demostrar.

\begin{proposition}
    Sean $\set{\vlst{x}{p}} \subseteq V$. Entonces son equivalentes:
    \begin{enumerate}[a)]
        \item $\set{\vlst{x}{p}}$ son linealmente independientes.
        \item Cualquier vector de $\lspan{\set{\vlst{x}{p}}}$ se puede expresar como una única combinación lineal.  Es decir, $\forall \vx \in \lspan{\set{\vlst{x}{p}}}, \spc \exists! \, \slst{a}{p} \in \K$ tal que $\vx = \lincomb{a}{x}{p}$. 
    \end{enumerate}
\end{proposition}

\begin{proof}

    Analicemos las dos implicaciones por separado.
    \begin{enumerate}[itemindent=20pt]
        \item[\protect\fbox{a) $\Rightarrow$ b)}] Supongamos que $\set{\vlst{x}{p}}$ son l.i. (linealmente independientes). \\
        Sea $\vx \in \lspan{\set{\vlst{x}{p}}} \Rightarrow \exists \slst{a}{p} \in \K \st \vx = \lincomb{a}{x}{p}$. Sean $\slst{a}{p} \in \K \st \vx = \lincomb{a}{x}{p}$. Entonces, restando:
        \begin{align*}
            \zv &= \vx - \vx = \lincomb{a}{x}{p} - (\lincomb{b}{x}{p}) = \\
                &= \parens{a^1-b^1}\vx_1 + \parens{a^2-b^2}\vx_2 + \dots + \parens{a^p-b^p}\vx_p
        \end{align*}
        Como hemos supuestos que $\set{\vlst{x}{p}}$ son l.i., por el corolario \ref{linear_independence_corollary} todos los coeficientes son 0, es decir,
        \[
            \begin{array}[t].{l}\}
            a^1-b^1 = 0 \\
            a^2-b^2 = 0 \\ 
            \hspace{12.74mm}\vdots \\
            a^p-b^p = 0
            \end{array}
            \begin{array}[t]{@{}c@{}}\\{}\implies {}\\{}\end{array}
            \begin{array}[t].{l}\}
                a^1 = b^1 \\
                a^2 = b^2 \\ 
                \hspace{5.3mm}\vdots \\
            a^p = b^p
            \end{array}
        \]
    
        \item[\protect\fbox{b) $\Rightarrow$ a)}] Supongamos que si $\vx \in \lspan{\set{\vlst{x}{p}}}, \spc \exists ! \, \slst{a}{p} \in \K $ tal que 
        \(\vx = \lincomb{a}{x}{p}\). Demostremos que $\set{\vlst{x}{p}}$ son linealmente independientes utilizando la caracterización. Sean $\slst{b}{p}\in \K$ tal que $\lincomb{b}{x}{p} = \zv$. Además, por ser $0\cdot \vx = \zv \spc \forall x \in V$, tenemos que $0 \cdot \vx_1 + 0 \cdot \vx_2 + \dots + 0 \cdot \vx_p = \zv$. El vector nulo se puede expresar entonces como dos combinaciones lineales:

        \[
        \begin{cases}
            \lincomb{b}{x}{p} &= \zv \\
            0 \cdot \vx_1 + 0 \cdot \vx_2 + \dots + 0 \cdot \vx_p &= \zv
        \end{cases}
        \]
        Como solamente existe una única combinación lineal que represente a $\zv$, entonces $b^i = 0 \spc \forall i \in \set{1, \dots, p} $ 
    \end{enumerate}
\end{proof}

    La siguiente proposición nos ayudará a expandir una familia de vectores que ya sabemos que son linealmente independientes con un nuevo vector bajo ciertas condiciones.

    \begin{proposition}
        Sean $\set{\vlst{x}{p}}$ linealmente independientes y sea $\vx \notin \lspan{\set{\vlst{x}{p}}}$, es decir, $\vx$ no es combinación lineal de $\set{\vlst{x}{p}}$. Entonces $\set{\vx, \vlst{x}{p}}$ son l.i.
    \end{proposition}

    \begin{proof}
        Sean $a,\slst{a}{p} \in \K$ tal que $a\vx + \lincomb{a}{x}{p} = 0$
        Veamos por reducción al absurdo que neecsariamente $a = 0$.
        \begin{enumerate}[itemindent=20pt]
            \item[\protect\fbox{si $a \ne 0$}] 
            \[
                \zv = \frac{1}{a} \cdot \zv = \frac{1}{a} \left( a\vx + a^1 \vx_1 + \dots + a^p\vx_p  \right) = \vx + \frac{a^1}{a}\vx_1 + \dots + \frac{a^p}{a}\vx_p
            \]
            Despejando $\vx$ restando
            \[
                \vx = \parens{\frac{-a^1}{a}}\vx_1 + \dots + \parens{\frac{-a^p}{a}}\vx_p
            \]
            Hemos llegado a una contradicción porque hemos podido expresar $\vx$ como combinación lineal de $\set{\vlst{x}{p}}$, por tanto, necesariamente $a=0$
            \item[\protect\fbox{si $a = 0$}] Como $a=0$, solamente necesitamos ver que $a_i=0 \spc \forall i \in \set{1, \dots, p}$.
            \[
                \zv = \underbrace{0 \cdot \vx}_{\zv} + \lincomb{a}{x}{p} = \lincomb{a}{x}{p}
            \] 

            Como $\set{\vlst{x}{p}}$ son l.i., $a_i=0 \spc \forall i \in \set{1, \dots, p}$. Como además $a=0$, tenemos que $\set{x, \vlst{x}{p}}$ son l.i.
        \end{enumerate}
    \end{proof}

    \begin{remark}
        A pesar de ser un error relativamente común, si existen 4 vectores linealmente independientes 3 a 3 entre sí, eso no implica que los 4 vectores sean linealmente independientes. Un contraejemplo de esto son los vectores $\set{(1,0,0),(0,1,0),(0,0,1),(1,1,1)}$ de $\R^3$.
    \end{remark}

\subsection{Teoremas de la base}

Hemos visto que si un conjunto de vectores es sistema generador, podemos expresar cualquier vector del espacio vectorial como una combinación lineal de elementos del conjunto. Además, si ese conjunto fuese linealmente independiente, podríamos expresar de manera única esa combinación lineal, lo cual es increíblemente útil.

\begin{definition}[Base]
    Una familia de vectores $\set{\vlst{u}{n}} \subseteq V$ es una base de $V$ si son un sistema de generadores de $V$ y son linealmente independientes.
\end{definition}

Como hemos dicho antes, cualquier vector se puede expresar de manera única como combinación lineal de vectores de la base. Más formalmente, si $\set{\vlst{u}{n}}$ es una base de $V$, Entonces
$$\forall \vx \in V, \spc \exists! \, \slst{a}{n} \in \K \st \vx = \lincomb{a}{u}{n}$$ 

\begin{remark}
    Una vez fijada una base, cada vector queda únicamente representado por los $n$ coeficientes únicos  $\slst{a}{n} \in \K$ de la combinación lineal. Podemos observar estos coeficientes como una tupla $\parens{\slst{a}{n}} \in \K^n$. Esto significa que independientemente de lo extraño que sea el espacio vectorial con el que estemos tratando, una vez fijada una base podemos trabajar con él \textit{como si fuera} $\K^n$. Formalmente esto se expresa creando una biyección $\phi$ desde $V$ hasta $\K^n$
    \begin{align*}
        \phi: V &\to \K^n \\
            \vx &\mapsto \parens{\slst{a}{n}}
    \end{align*} 
    siendo $\slst{a}{n}$ los únicos escalares tal que $\vx = \lincomb{a}{u}{n}$.
\end{remark}

Gracias a las buenas propiedades de las bases, sería muy afortunado que todo espacio vectorial finitamente generado tuviese una. Eso es precisamente lo que nos dice el primer teorema de la base.

\begin{theorem}[Primer teorema de la base]
    Sea $V \ne \set{\zv}$ finitamente generado, entonces $\exists \set{\vlst{u}{n}}$ base de $V$ 
\end{theorem}

\begin{proof}
    Si $V \ne \set{\zv}$ es finitamente generado, $\exists p \in \N, \spc \exists \vlst{x}{p} \in V$ tal que $\vlst{x}{p}$ es sistema de generadores de $V$. Pueden darse dos casos, si $\set{\vlst{x}{p}}$ son linealmente independientes, entonces son base y hemos demostrado lo que queríamos. En caso contrario, son linealmente dependientes, es decir
    \[
        \exists i \in \set{1, \dots, p} \st \vx_i \in \lspan{\set{\vlst{x}{i-1}, \vx_{i+1}, \dots, \vx_{p}}}
    \]
    Veamos que entonces $\set{\vx_{1}, \dots,  \vx_{i-1}, \vx_{i+1}, \dots, \vx_{p}}$ también es sistema de generadores de $V$. $\vx_i$ es por tanto combinación lineal del resto, por lo que $\exists \islst{a}{p}{i} \in \K$ tal que $\vx_i = \ilincomb{a}{x}{p}{i}$. Por ser $\set{\vlst{x}{p}}$ sistema de generadores, $\forall \vx \in V, \spc \exists \cslst{b}{p}{i} \in \K $ tal que 
    \begin{align*}
        \vx &= \clincomb{b}{x}{p}{i} \\
            &= b^1\vx_{1} + \dots + b^{i-1} \vx_{i-1} + b^i \parens{\ilincomb{a}{x}{p}{i}} + \\
            &\hspace{11.35mm} + b^{i+1} \vx_{i+1} + \dots + b^p \vx_p = \\
            &= \parens{b^1 + b^i a^1}\vx_{1} + \dots + \parens{b^{i-1} + b^i a^{i-1}}\vx_{i-1} + \parens{b^{i+1} + b^i a^{i-1}}\vx_{i+1} + \dots + \parens{b^p + b^i a^p}\vx_{p}
    \end{align*}
    Por tanto, podemos expresar cada $\vx$ como combinación lineal de $\set{\ivlst{x}{p}{i}}$, por lo que $\set{\ivlst{x}{p}{i}}$ es sistema de generadores de $V$. 
    Podemos repetir este método hasta que todos los vectores sean linealmente independientes. En el peor caso, aplicando este método $p-1$ veces, llegaremos a $\set{\vx_j}$. Como $V \ne \set{\zv}$, entonces $\vx_j \ne \zv \Rightarrow \set{\vx_j}$ es linealmente independiente.
\end{proof}

Demostraremos ahora un teorema, que a pesar de parecer tener un enunciado un tanto árido, nos permitirá deducir con facilidad que todas las bases de un espacio vectorial finitamente generado poseen el mismo número de vectores.

\begin{theorem}[Steinitz]
    Sea $\set{\vlst{x}{p}}$ un sistema de generadores de $V$ y consideremos $\set{\vlst{y}{q}}$ linealmente independientes. Entonces $q \le p$ y se puede obtener un nuevo sistema de generadores sustituyendo $q$ de los vectores $\vx_i$ por los $q$ vectores $\vlst{y}{q}$.
\end{theorem}

\begin{proof}
    Demostremos primero la segunda parte por inducción.
    \begin{enumerate}[itemindent=30pt]
        \item[\protect\fbox{Caso base}] $ $ \linebreak
    \end{enumerate}
    Como $\vy_1 \in V$ y $\set{\vlst{x}{p}}$ es sistema de generadores, entonces $\exists \slst{a}{p} \in \K$ tal que $\vy_1 = \lincomb{a}{x}{p}$. Como $\vy_1$ forma parte de una familia de vectores l.i., entonces $\parens{\slst{a}{p}} \ne \parens{0, 0, \dots, 0}$, es decir, no todas las $a^i$ son nulas. Sin pérdida de generalidad (podemos permutar los vectores $\vx_j$ para llevar el escalar no nulo hasta $a^1$), suponemos que $a^1\ne 0$. Por tanto,
    \begin{align*}
        \frac{1}{a^1}\vy_1 &= \frac{1}{a^1} \parens{\lincomb{a}{x}{p}} \\
                        &= \underbrace{\frac{a^1}{a^1}}_{1}\vx_1 + \frac{a^2}{a^1}\vx_2 + \dots + \frac{a^p}{a^1}\vx_p 
    \end{align*}
    Despejamos $\vx_1$ restando
    \[
        \vx_1 = \frac{1}{a^1}\vy_{1} + \frac{-a^2}{a^1}\vx_2 + \dots + \frac{-a^p}{a^1}\vx_p
    \]
    Veamos que $\set{\vy_1, \vx_2, \dots, \vx_p}$ es sistema de generadores de $V$. $\forall \vx \in V, \spc \exists \slst{\alpha}{p} \in \K$ tal que 
    \begin{align*}
        \vx &= \lincomb{\alpha}{x}{p} \\
            &= \alpha^1 \parens{\frac{1}{a^1}\vy_{1} + \frac{-a^2}{a^1}\vx_2 + \dots + \frac{-a^p}{a^1}\vx_p} + \alpha^2\vx_2 + \dots + \alpha^p\vx_p \\
            &= \frac{\alpha^1}{a^1}\vy_1 + \parens{\alpha^2 - \frac{a^2 \alpha^1}{a^1}}\vx_2 + \dots + \parens{\alpha^p - \frac{a^p \alpha^1}{a^1}}\vx_p
    \end{align*}
    Por tanto $\set{\vy_1, \vx_2, \dots, \vx_p}$ es sistema de generadores de $V$.
    \begin{enumerate}[itemindent=50pt]
        \item[\hspace{20mm}\protect\fbox{Paso inductivo}] $ $ \linebreak
    \end{enumerate}
    Supongamos que para $i < \min \braces{p, q}$ tenemos que $\set{\vy_1, \vy_2, \dots, \vy_{i}, \vx_{i+1}, \dots, \vx_p}$ es sistema de generadores de $V$. Como $\vy_{i+1} \in V, \spc \exists \slst{a}{i}, b^{i+1}, \dots, b^q \in \K$ tales que 
    \[
        \vy_{i+1} = \lincomb{a}{y}{i} + b^{i+1}\vx_{i+1} + \dots + b^p \vx_p 
    \]
    De nuevo, por formar $\vy_{i+1}$ parte de una familia l.i., $\parens{b^{i+1}, \dots, b^p} \ne \parens{0, \dots, 0}$. Sin pérdida de generalidad, supongamos que $b^{i+1} \ne 0$, por lo que
    \begin{align*}
        \frac{1}{b^{i+1}}\vy_{i+1} &= \frac{1}{b^{i+1}} \parens{\lincomb{a}{y}{i} + b^{i+1}\vx_{i+1} + \dots + b^p \vx_p} \\
                                   &= \frac{a^1}{b^{i+1}} \vy_1 + \frac{a^2}{b^{i+1}} \vy_2 + \dots + \frac{a^i}{b^{i+1}}\vy_i + \underbrace{\frac{b^{i+1}}{b^{i+1}}}_{1}\vx_{i+1} + \dots + \frac{b^{p}}{b^{i+1}}\vx_{p} 
    \end{align*}
    Despejando $\vx_{i+1}$
    \[
        \vx_{i+1} = \frac{-a^1}{b^{i+1}}\vy_1 + \frac{-a^2}{b^{i+1}}\vy_2 + \dots + \frac{-a^i}{b^{i+1}}\vy_i + \frac{1}{b^{i+1}} \vy_{i+1} + \frac{-b^{i+2}}{b^{i+1}}\vx_{i+2} +  \dots + \frac{-b^p}{b^{i+1}}\vx_p
    \]
    Vamos a demostrar que $\set{\vy_1, \vy_2, \dots, \vy_{i}, \vy_{i+1}, \dots, \vx_p}$ es sistema de generadores de $V$. Sea $\vx \in V, \spc \exists \slst{\alpha}{i},\beta^{i+1}, \dots, \beta^p \in \K$ tal que 
    \begin{align*}
        \vx &= \lincomb{\alpha}{y}{i} + \beta^{i+1}\vx_{i+1}+\dots  + \beta^p\vx_p \\
            &= \lincomb{\alpha}{y}{i} + \\
            &\hspace{5mm} + \beta^{i+1} \parens{\frac{-a^1}{b^{i+1}}\vy_1 + \frac{-a^2}{b^{i+1}}\vy_2 + \dots + \frac{-a^i}{b^{i+1}}\vy_i + \frac{1}{b^{i+1}} \vy_{i+1} + \frac{-b^{i+2}}{b^{i+1}}\vx_{i+2} +  \dots + \frac{-b^p}{b^{i+1}}\vx_p} + \\
            &\hspace{5mm} + \dots + \beta^p\vx_{p} \\
            &= \parens{\alpha^1 - \frac{\beta^{i+1}a^1}{b^{i+1}}} \vy_1 + \dots + \parens{\alpha^i - \frac{\beta^{i+1}a^i}{b^{i+1}}}\vy_i + \frac{\beta^{i+1}}{b^{i+1}}\vy_{i+1} + \dots + \parens{\beta^p - \frac{\beta^{i+1}b^p}{b^{i+1}}}\vx_p
    \end{align*}
    Por tanto, todos los vectores son combinación lineal de $\set{\vy_1, \vy_2, \dots, \vy_{i}, \vy_{i+1}, \dots, \vx_p}$, por lo que es sistema de generadores.
    \begin{enumerate}[itemindent=21pt]
        \item[\hspace{20mm}\protect\fbox{Si $p < q$}] $ $ \linebreak
    \end{enumerate}
    Demostramos por último la primera parte del teorema por reducción al absurdo. Supongamos que $p > q$. Sabemos que $\set{\vlst{y}{p}, \vy_{p+1}, \dots, \vy_{q}}$ son linealmente independientes. Como $\set{\vlst{x}{p}}$ es sistema de generadores, sustituyendo como antes, tenemos que $\set{\vlst{y}{p}}$ es sistema de generadores.  Sin embargo, por ser los $\vy_j$ linealmente independientes, $\vy_{i+1} \notin \lspan{\set{\vlst{y}{p}}}$. Hemos llegado a una contradicción, por lo que $p \ge q$.
\end{proof}

Estamos por fin en condiciones de demostrar que todas las bases tienen el mismo número de elementos.

\begin{corollary}[Segundo teorema de la base]
    Todas las bases de un espacio vectorial $V \ne {\zv}$ finitamente generado tienen el mismo número de elementos.
\end{corollary}

\begin{proof}
    Sean $\mathcal{B} = \set{\vlst{u}{n}}$ y $\mathcal{B'} = \set{\vlst{v}{m}}$ dos bases de $V$. Aplicamos dos veces el teorema de Steinitz:
    \begin{itemize}
        \item $\mathcal{B}$ es sistema de generadores y $\mathcal{B'}$ es una familia l.i. Aplicando Steinitz, $m \le n$
        \item $\mathcal{B'}$ es sistema de generadores y $\mathcal{B}$ es una familia l.i. Aplicando Steinitz, $n \le m$
    \end{itemize}

    Por tanto, como $n \le m$ y $m \le n$, tenemos que $m = n$
\end{proof}

Por tanto, el número de elementos de la base es algo inherente al espacio vectorial, y no a cada base, lo que motiva la siguiente definicion.

\begin{definition}[Dimensión]
    Llamaremos dimensión de $V \ne \set{\zv}$ al número de vectores de cualquier base de $V$. Lo notaremos como $\dim{V}$. Diremos que $\dim{\set{\zv}} = 0$.
\end{definition}

\begin{remark}
    Si $\dim{V} = n$ y $\set{\vlst{x}{p}} \subseteq V$ linealmente independientes, aplicando Steinitz tenemos que $p \le n$. Es decir, $\dim{V}$ es el número máximo de vectores que forman una familia linealmente independiente. Análogamente, si $\set{\vlst{x}{p}} \subseteq V$ sistema de geeradores, aplicando Steinitz tenemos que $n \le p$. Es decir, $\dim{V}$ es el mínimo número de vectores que forman un sistema de generadores de $V$. 
\end{remark}

\begin{example}
    La dimensión de un espacio puede cambiar dependiendo del cuerpo con el que se considere el espacio vectorial. Por ejemplo $\Complex$ es un $\R$-espacio vectorial y un $\Complex$-espacio vectorial y sus dimensiones son distintas:
    \begin{itemize}
        \item $\mathrm{dim}_{\Complex}\parens{\Complex} = 1$, tomando como base $\mathcal{B} = \set{1}$
        \item $\mathrm{dim}_{\R}\parens{\Complex} = 1$, tomando como base $\mathcal{B'} = \set{1, i}$
    \end{itemize}
\end{example}

Siguiendo con este tipo de razonamientos, demostraremos que si tenemos una familia sistema de generadores o linealmente independiente y además tiene el mismo número de elementos que la dimensión, entonces es base.

\begin{corollary}
    Se cumplen las siguientes afirmaciones:
    \begin{enumerate}[a)]
        \item Si $\dim{V} = n$ y $\set{\vlst{u}{n}}$ es una familia linealmente independiente, entonces $\set{\vlst{u}{n}}$ es base de $V$. 
        \item Si $\set{\vlst{v}{n}}$ es sistema de generadores de $V$, entonces $\set{\vlst{u}{n}}$ es base.
    \end{enumerate}
\end{corollary}

\begin{proof}
    Demostramos individualmente cada afirmación.
    \begin{enumerate}[a)]
        \item Si $\dim{V} = n$, entonces $\exists \set{\vlst{x}{n}}$ base (sistema de generadores y linealmente independiente). Podemos sustituir $\set{\vlst{x}{n}}$ por $\set{\vlst{u}{n}}$ y tendremos un nuevo sistema de generadores. Como $\set{\vlst{u}{n}}$ es linealmente independiente, entonces es base.
        \item Como todos los sistemas de generadores tienen una base dentro y todas las bases tienen $n$ elementos, si $\set{\vlst{v}{n}}$ es sistema de generadores, también es base.
    \end{enumerate}
\end{proof}

Intuitivamente, un subespacio vectorial no puede ser más grande que el espacio que lo contiene. Esta afirmación se formaliza en la siguiente proposición.

\begin{proposition}
    Sea $L \subseteq V$ un subespacio vectorial. Entonces $L$ es finitamente generado y $\dim{L} \le \dim{V}$. Además, $\dim{L}=\dim{V} \iff L = V$.
\end{proposition}

\begin{proof}
    Para el además, si $\dim{L} = n$ y $\set{\vlst{x}{n}} \subseteq L \subseteq V$ es base de $L$, entonces los vectores son linealmente independientes. Como además $n = \dim{L} = \dim{V}$, entonces $\set{\vlst{x}{n}}$ es base de $V$. Como $L$ y $V$ tienen la misma base, entonces $L=V=\lspan{\set{\vlst{x}{n}}}$.

    Demostremos ahora la primera parte del teorema.  Se tiene que dar o $L = \zvs$ o $\exists \vx_1 \in L \setminus \zvs$ y $\set{\vx_1} \subseteq L$ l.i. . En el segundo caso, o $L=\lspan{\set{\vx_1}}$ o $\exists \vx_2 \in L \setminus \lspan{\set{\vx_1}}$ y $\set{\vx_1, \vx_2} \subseteq L$ l.i. . De nuevo, en el segundo caso, o $L=\lspan{\set{\vx_1, \vx_2}}$ o $\exists \vx_3 \in L \setminus \lspan{\set{\vx_1, \vx_2}}$ y $\set{\vx_1, \vx_2, \vx_3} \subseteq L$ l.i. Generalizando el argumento, supuestos definidos $\set{\vlst{x}{p}} \subseteq L$ l.i. se tiene que o $L = \lspan{\vlst{x}{p}}$ o $\exists \vx_{p+1} \in L \setminus \set{\vlst{x}{p}, \vx_{p+1}}$ y $\set{\vlst{x}{p}, \vx_{p+1}} \subseteq L \subseteq V$ l.i. Forzosamente debemos acabar como muy tarde al tener $n$ elementos, pues tenemos un conjunto linealmente independiente contenido en $V$, cuya dimensión es $n$, por lo que el número máximo de vectores linealmente independientes es $n$. En el $m \le n$ que acabemos, hemos demostrado que $L = \lspan{\vlst{x}{m}}$, por lo que $\dim{L} = m \le n = \dim{V}$.
\end{proof}

El último resultado que estudiaremos sobre bases trata sobre la ampliación de un conjunto linealmente independiente (de tamaño menor o igual que la dimensión de $V$) a una base de $V$ añadiendo elementos.
\begin{theorem}[Teorema de extensión de la base]   
    Si $\set{\vlst{u}{p}} \subseteq V$ es linealmente independiente, entonces  $\exists \set{\vu_{p+1}, \dots, \vu_n} \subseteq V$ tal que $\set{\vlst{u}{p}, \vu_{p+1}, \dots, \vu_n}$ es base de $V$. Es decir, si $\set{\vlst{u}{p}} \subseteq V$ es base de un subespacio vectorial de $V$, $\exists \set{\vu_{p+1}, \dots, \vu_n} \subseteq V$ tal que $\set{\vlst{u}{p}, \vu_{p+1}, \dots, \vu_n}$ es base de $V$.
\end{theorem}
\begin{proof}
    Sea $\mathcal{B} = \set{\vlst{v}{p}, \vv_{p+1}, \dots, \vv_{n}}$ una base de $V$, por lo que es un sistema de generadores. Por Steinitz, podemos sustituir una familia de vectores linealmente independientes en la base, por lo que $\exists \vu_{p+1}, \dots, \vu_n \in \mathcal{B}$ tal que  $\set{\vlst{u}{p}, \vu_{p+1}, \dots, \vu_n}$ es sistema de generadores de $V$. Como es sistema de generadores y tiene $n = \dim{V}$ elementos, es una base.
\end{proof}

\section{Operaciones con subespacios vectoriales}

\end{document}