% !TeX root = ../algebra_lineal.tex

\documentclass[../algebra_lineal.tex]{subfiles}

\begin{document}

\section{Notación}

¿Explicar aquí notación del tal que (aunque estoy poniendo tal que todo el rato con palabras)? ¿Explicar aquí la notación del subset (aunque estoy intentando utilizar todo el rato subseteq)? ¿Explicar aquí que los superíndices no son potencias? ¿Introducción a la importancia del álgebra lineal?

\section{Cuerpos}

Para formalizar el concepto de espacio vectorial, deberemos antes formalizar el conjunto de escalares, es decir, un conjunto ``de dimensión uno'', signifique lo que signifique eso, que podremos utilizar para multiplicar (``escalar'') vectores. Habitualmente usaremos $\R$ o $\Complex$, pero en esta sección vamos a intentar extraer las propiedades relevantes de estos dos conjuntos para definir una estructura algebraica abstracta, el \textit{cuerpo}, del que únicamente conoceremos sus propiedades.

Informalmente, las propiedades más relevantes son la existencia de las operaciones suma y multiplicación, que estas sean algebraicamente cerradas
(es decir, que al sumar o multiplicar dos elementos del cuerpo obtengamos otro elemento del cuerpo), que cumplan las propiedades asociativa, conmutativa
\footnote{En ocasiones se distinguen a los cuerpos que cumplen la propiedad conmutativa como \textit{cuerpos abelianos}, en este texto todos los cuerpos
 serán conmutativos} y distributiva, que existan elementos neutros (0 para la suma y 1 para la multiplicación), que exista el concepto de \textit{división} (es decir, que cada elemento tenga un inverso multiplicativo), el concepto de \textit{resta} (es decir, que cada elemento tenga un opuesto aditivo).

Analicemos ahora la definición formal.

\begin{definition}[Cuerpo]
    Decimos que $\K$ es un cuerpo si existen dos aplicaciones $+:\K \times \K \to \K$, $\cdot:\K \times \K \to \K$ llamadas respectivamente suma y multiplicación que satisfacen las siguientes propiedades:
    \begin{enumerate}
        \item Asociatividad de la suma y multiplicación: $a+(b+c)=(a+b)+c$, $a\cdot(b\cdot c)=(a\cdot b)\cdot c$
        \item Conmutatividad de la suma y multiplicación: $a+b=b+a$, $a\cdot b = b \cdot a$
        \item Distributividad de la multiplicación sobre la suma: $a \cdot (b+c) = (a \cdot b) + (a \cdot c)$
        \item Elemento neutro para la suma: $\exists 0 \in \K \st \forall a \in \K, \spc a+0=a$
        \item Elemento identidad para la multiplicación:  $\exists 1 \in \K \st \forall a \in \K, \spc a\cdot 1=a$
        \item \label{cuerpo_opuesto} Opuesto aditivo: $\forall a \in \K, \spc \exists {(-a)} \in \K \st a + (-a) = 0 $.
        \item \label{cuerpo_inverso} Inverso multiplicativo: $\forall a \neq 0 \in \K, \spc \exists {(a^{-1})} \in \K \st a \cdot (a^{-1}) = 1$
    \end{enumerate}    
\end{definition}

\begin{notation}
    En las propiedades \ref{cuerpo_opuesto} y \ref{cuerpo_inverso} de los opuestos e inversos, hemos realizado un pequeño abuso de notación por motivos de claridad. Lo que realmente indica la propiedad es que para cualquier $a \in \K$ existe otro elemento $b_a \in \K$ (indicamos con el subíndice que depende de $a$) tal que $a + b_a = 0$. Denotamos al elemento $b_a$ como $-a$. Análogamente hacemos lo propio con el inverso multiplicativo. 
\end{notation}

\begin{remark}
    Sea cual sea el cuerpo, contiene a todos los números naturales ya que si $n \in \N \implies n = 1 + 1 + \dots + 1 \in \K$ porque cada uno de los $1$ pertenece a $\K$, por lo que la suma pertenece también.
\end{remark}

Con esta definición de cuerpo hemos ampliado significativamente el rango de conjuntos que podemos usar. Sin embargo, surgen algunos casos patológicos, como por ejemplo $\Z_2 = \set{0, 1}$, el cuerpo de los enteros módulo 2 (es decir, se ejecuta la aritmética usualmente y finalmente colapsamos los pares al 0 y los impares al 1). Se deja como ejercicio al lector demostrar que efectivamente $\Z_2$ es un cuerpo (de hecho, $\Z_p$ es un cuerpo para todo $p$ primo, algo fuera del alcance de este texto). La patología de $\Z_2$ radica en que $1+1=0$. A menudo querremos dividir entre 2 en variadas demostraciones, por lo que impondremos un requisito especial a los cuerpos que utilizaremos. Para ello, debemos introducir la definición de \textit{característica}.

\begin{definition}
    Llamamos característica de $\K$ al mínimo número de veces que es necesario sumar el elemento identidad 1 en una suma para obtener el elemento neutro 0. Es decir, la característica es el entero positivo $n$ más pequeño tal que:
    \[
        \underbrace{1+\dots+1}_{n\, \, \mathrm{sumandos}} = 0
    \]  
\end{definition}

A partir de ahora, salvo que mencionemos expresamente lo contrario, siempre hablaremos de cuerpos $\K$ de característica distinta de 2 (es decir, $1+1 \ne 0$).

\section{Espacios vectoriales}

Al igual que hicimos con los cuerpos, vamos a definir los espacios vectoriales usando únicamente las propiedades que lo definen. Más adelante relacionaremos esta definición con el concepto intuitivo de espacio vectorial utilizando coordenadas.

Al contrario que hicimos con los cuerpos, para definir un espacio vectorial necesitaremos un cuerpo $\K$ de característica distinta de 2, que utilizaremos para multiplicar los vectores, por lo que el cuerpo aparecerá en la definición de espacio vectorial.

\begin{definition}[Espacio vectorial]
    \label{vector_space_definition}
    Un conjunto $V \neq \emptyset$ es un espacio vectorial sobre $\K$ (también diremos que es un $\K$-espacio vectorial) si existen las siguientes operaciones en $V$ (suma y multiplicación por escalar) 
    
    \begin{multicols}{2}
        \noindent
        \begin{align*} +: V \times V &\to V \\
                (\vx, \vy) &\mapsto \vx + \vy
        \end{align*}
        \begin{align*}
            \cdot: \K \times V &\to V \\
                (a, \vx) &\mapsto a \vx
        \end{align*}
    \end{multicols}

    y satisfacen las siguientes propiedades:

    \begin{enumerate}
        \item Conmutatividad de la suma: $\forall \vx, \vy \in V, \spc \vx + \vy = \vy + \vx$
        \item Asociatividad de la suma: $\forall \vx, \vy, \vz \in V, \spc \vx + (\vy + \vz) = (\vx + \vy) + \vz$
        \item \label{espacio_vectorial_neutro} Elemento neutro: $\exists \zv \in V \st \forall \vx \in V, \spc \vx + \zv = \vx$
        \item \label{espacio_vectorial_opuesto} Elemento opuesto: $\forall \vx \in V, \spc \exists {({-x})} \in V \st  \vx + (-\vx) = \zv$
        \item Distributiva vectorial: $\forall a \in \K, \spc \forall \vx, \vy \in V, \spc a(\vx + \vy) = a\vx + a \vy$
        \item Distributiva escalar: $\forall a, b\in \K, \spc \forall \vx \in V, \spc (a+b)\vx = a\vx + b\vx$ 
        \item Asociativa: $\forall a, b \in \K, \spc \forall \vx \in V, \spc a \cdot (b \cdot \vx) = (a\cdot b) \cdot \vx $
        \item \label{espacio_vectorial_unidad} Elemento unidad: $\forall \vx \in V, \spc 1 \cdot \vx = \vx$ 
    \end{enumerate}

\end{definition}

\begin{remark}
    Es importante distinguir el elemento neutro $0 \in \K$ del cuerpo escalar, del elemento neutro $\zv \in V$ del espacio vectorial, ya que uno es un vector y otro un escalar. Sin embargo, el elemento identidad $1 \in \K$ de la propiedad \ref{espacio_vectorial_unidad} sí que es un escalar (se suman vectores, y se multiplican vectores con escalares).  
\end{remark}

\begin{notation}
    Si $V$ es un \kvspace, llamaremos a los elementos de V \textit{vectores} y a los elementos de $\K$ escalares.
\end{notation}

A partir de ahora, si no indicamos otra cosa, $V$ será un \kvspace, siendo $\K$ un cuerpo de característica distinta de 2.

Es posible que esta definición abstracta de espacio vectorial parezca al lector algo árida, sin embargo es una herramienta muy potente que nos permitirá desarrollar una teoría que se puede aplicar a cualquier conjunto que satisfaga la definición de espacio vectorial, no únicamente a $\K^n$. Ejemplos de espacios vectoriales muy útiles en las matemáticas son los polinomios $\K[x]$ \footnote{Denotaremos como $\K[x]$ el conjunto de polinomios de cualquier grado sobre la variable $x$, es decir, polinomios de la forma, $a_n x^n + \dots + a_1 x + a_0$ con $a_i \in \K \spc \forall i \in {1, \dots, n}$}, las funciones continuas, las soluciones de ecuaciones diferenciales lineales, etc. Si desarrollamos todo desde las propiedades de espacio vectorial, podremos aplicar toda la teoría que desarrollemos a todos esos espacios vectoriales y muchos otros, sin tener que distinguir casos durante el desarrollo de la teoría.

A pesar de la potencia de esta aproximación a la definición de espacio vectorial, debemos demostrar incluso las cosas más nimias desde las propiedades. Comenzamos demostrando algunas propiedades inmediatas.

\begin{proposition}
    Sea $V$ un \kvspace. Entonces, las siguientes propiedades son ciertas:
    \begin{enumerate}
        \item $a \cdot \zv = \zv \spc \forall a \in \K$
        \item $a(-\vx) = -a\cdot \vx \spc \forall a \in \K$
        \item $0 \cdot \vx = \zv \spc \forall \vx \in V$
        \item \label{propiedad_cuatro_espacios_vectoriales} $(-a)\cdot \vx = -a\vx$
        \item Si $\vx \ne 0$, entonces $a \cdot \vx = \zv \implies a = 0$
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}[wide, labelwidth=0pt, labelindent=0pt]
        \item Primero observamos que $a\cdot \zv = a(\zv + \zv) = a \cdot \zv + a \cdot \zv $. Restando a ambos lados $a\cdot \zv$ obtenemos que 
            \[\zv = a\cdot \zv + (-a\cdot \zv) = (a \cdot \zv + a \cdot \zv) + (-a \cdot \zv) =  a \cdot \zv + \underbrace{(a \cdot \zv + (-a \cdot \zv))}_{\zv} = a \cdot \zv \]
        \item Primero nos percatamos de que $\zv = a \cdot \zv = a(\vx + (-\vx)) = a\vx +a\cdot(-\vx)$. Como $\zv$ es igual a dicha expresión, podemos sustituir la expresión de la derecha en cualquier lugar en el que aparezca $\zv$ por lo que
         \[ -a\vx = -a\vx + \zv = -a\vx + (a\vx + a(-\vx)) = \underbrace{(-a\vx + a\vx)}_{\zv} + a(-\vx) = a(-\vx) \] 
        \item En primer lugar vemos que $0 \cdot \vx = (0+0)\vx = 0 \cdot \vx + 0 \cdot \vx$. De nuevo, podremos sustituir eso en la siguiente ecuación
        \[\zv  = 0 \cdot \vx +(-0\cdot \vx) = (0\cdot \vx + 0\cdot \vx) + (-0\cdot \vx) =  0\cdot \vx + \underbrace{(0\cdot \vx + (-0\cdot \vx))}_{\zv} = 0\cdot\vx\]
        \item Observamos que $\zv = 0 \cdot \vx = (a + (-a))\vx = a\vx + (-a)\vx$. Sumar $\zv$ es equivalente a no hacer nada, por lo que podremos sumar la expresión de la derecha sin cambiar ningún vector
        \[-a\vx = -a\vx + (a\vx + (-a)\vx) = \underbrace{(-a\vx + a\vx)}_{\zv} + (-a)\vx = (-a)\vx \]
        \item Lo demostraremos por reducción al absurdo. Supongamos que $a \neq 0$, entonces $\frac{1}{a} \in \K$. Por tanto
        \[\vx = \underbrace{\left(\frac{a}{a} \right)}_{1} \vx = \frac{1}{a} (a \cdot \vx) = \frac{1}{a} \cdot \zv = \zv\]
        Hemos llegado a que $\vx = 0$, lo que es una contradicción.
    \end{enumerate}
\end{proof}

\begin{notation}
    Utilizaremos la notación usual en la que restar significa sumar el opuesto, es decir, $\vx - \vy = \vx + (-\vy)$
\end{notation}

\begin{remark}
    Si particularizamos la propiedad \ref{propiedad_cuatro_espacios_vectoriales} para $a=1$, obtenemos que $-\vx = (-1)\vx$, por lo que para obtener el vector opuesto basta con multiplicar por el opuesto del escalar $1$ (es decir, por $-1$).
\end{remark}

\section{Subespacios vectoriales}

En matemáticas es muy común querer trabajar con subconjuntos de un conjunto más grande. Habrá ocasiones en las que nos convenga trabajar por ejemplo con rectas o planos en vez de con el espacio vectorial total. Sin embargo, queremos que esos subconjuntos sigan teniendo la estructura de espacio vectorial, para poder aplicar a ellos también toda la teoría que estamos desarrollando. Nace de ahí el concepto de \textit{subespacio vectorial}, que informalmente es un subconjunto que con las \textbf{operaciones inducidas} del espacio vectorial que le contiene, sigue siendo a su vez un espacio vectorial. Es muy importante que las operaciones sigan siendo las mismas, es decir, la suma es la misma que en el espacio vectorial \textit{ambiente} (el espacio vectorial que lo contiene), para después poder juntar y usar varios subespacios vectoriales a la vez.

Para que las operaciones inducidas puedan ser efectivamente las operaciones de un espacio vectorial, estas deben ser cerradas. Es decir, la suma de vectores en el subespacio vectorial debe de seguir estando en el subespacio vectorial, y la multiplicación de un vector del subespacio por un escalar debe seguir estando en el subespacio. Esto es una condición necesaria para que un subconjunto sea subespacio vectorial, y es tan importante que hasta merece nombre.

\begin{definition}
    Un subconjunto no vacío $L\subseteq V$ es parte estable si verifica:
    \begin{enumerate}
        \item $\forall \vx, \vy \in L \implies \vx + \vy \in L$
        \item $\forall \vx \in L, \spc \forall a \in \K \implies a\cdot \vx \in L$
    \end{enumerate}
\end{definition}

Estamos ahora en condiciones de definir qué es un subespacio vectorial (sabiendo que ser parte estable es condición necesaria).

\begin{definition}[Subespacio vectorial]
    Una parte estable $L$ de $V$ es un subespacio vectorial de $V$ si $L$ con las operaciones inducidas por las de $V$ en $l$ es un \kvspace.
\end{definition}

\begin{remark}
    Las operaciones inducidas por $V$ en $L$ están bien definidas únicamente si $L$ es parte estable, y son las siguientes:
    \begin{multicols}{2}
        \noindent
        \begin{align*} +: L \times L &\to L \\
                (\vx, \vy) &\mapsto \vx + \vy \in L
        \end{align*}
        \noindent
        \begin{align*}
            \cdot: \K \times L &\to L \\
                (a, \vx) &\mapsto a \vx \in L
        \end{align*}
    \end{multicols} 
\end{remark}

De hecho, ser parte estable no es solamente condición necesaria, sino que también es condición suficiente.

\begin{proposition}
    $L$ es parte estable si y solamente si $L$ es un subespacio vectorial
\end{proposition}

\begin{proof}
    Solamente debemos demostrar la suficiencia. Sea $L \subseteq V$ parte estable. Únicamente debemos demostrar las propiedades \ref{espacio_vectorial_neutro} y \ref{espacio_vectorial_opuesto} de la definición \ref{vector_space_definition} de espacio vectorial para ver que $L$ es un \kvspace, ya que el resto de propiedades se cumplen en $V$, por lo que particularmente se cumplen en $L$.
    
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item Sea $\vx \in L$, entonces $-\vx = \underbrace{(-1)}_{\in \K} \cdot \underbrace{\vx}_{\in V} \in L$. Pertenece a $L$ por ser $L$ parte estable.
        \setcounter{enumi}{2}
        \item Sea $\vx \in L$, entonces $\underbrace{\vx}_{\in L} + \underbrace{(-\vx)}_{\in L} = \zv \in L$. De nuevo pertenece a $L$ por ser $L$ parte estable.
    \end{enumerate}
    $ $
\end{proof}

Las operaciones de las que disponen los espacios son la suma de vectores y multiplicación por escalares, que usualmente combinaremos para dar lugar a las \textit{combinaciones lineales}

\begin{definition}[Combinación lineal]
    \label{definicion_combinacion_lineal}
    Sean $p \in \N, \spc \dlst{\vx}{p} \in V, \spc \ulst{a}{p} \in \K$. Si $\vx = \lincomb{a}{x}{p}$, diremos que $\vx$ es combinación linear de $\dlst{x}{p}$ con coeficientes $\ulst{a}{p}$.

\end{definition}

\begin{remark}
    En la definición \ref{definicion_combinacion_lineal} los superíndices no indican exponentes ni potencias, sino que se usan para distinguir distintos escalares. Como regla general, indexaremos los escalares por superíndices y los vectores con subíndices por motivos de claridad.
\end{remark}

Las combinaciones lineales son especialmente importantes porque juntan de manera concisa las sumas y multiplicaciones por escalares. Permiten usualmente expresar las propiedades que normalmente se deberían enunciar por separado en una sola propiedad, utilizando combinaciones lineales en vez de sumas y multiplicaciones. 

El primer ejemplo de esto es que los subespacios vectoriales son los conjuntos que continenen a las combinaciones lineales de sus vectores.


\begin{proposition}
    $\emptyset \ne L \subseteq V$ es subespacio vectorial $\iff \forall p \in N, \spc \forall \dlst{x}{p} \in L, \spc \forall \ulst{a}{p} \in \K, \spc \lincomb{a}{x}{p} \in L$.
\end{proposition}

\begin{proof}
    Veamos las dos implicaciones por separado.
    \begin{enumerate}
        \item[\protect\fbox{$\Rightarrow$}] Supongamos que $L$ es subespacio vectorial y sea $p \in \N$ \\
        Sean $\dlst{x}{p} \in L, \spc \ulst{a}{p} \in \K, \spc \lincomb{a}{x}{p} \in L$. Entonces, $a^1 \vx_1, a^2\vx_2, \dots, a^p\vx_p \in L$ por ser $L$ subespacio vectorial, y de nuevo, por ser $L$ subespacio vectorial se da que $\lincomb{a}{x}{p} \in L$. 
        \item[\protect\fbox{$\Leftarrow$}] Si $L$ contiene las combinaciones lineales, particularmente contiene cada suma y cada multiplicación por escalar, así que es parte estable y por tanto espacio vectorial.
    \end{enumerate} 
\end{proof}

\section{Bases de espacios vectoriales}

No todos los subconjuntos son espacios vectoriales, pero nos puede interesar encontrar un subespacio vectorial que contenga a determinado conjunto sin que este sea el total. Después de introducir las combinaciones lineales en la sección anterior, no es descabellada la idea de definir el conjunto de todas las combinaciones lineales de otro conjunto.

\begin{definition}
    Sea $\emptyset \neq H \subseteq V$ un subconjunto de $V$ que puede ser o no ser un espacio vectorial. Definimos $\lspan{H}$ como el conjunto de todas las combinaciones lineales de $H$, es decir,
    \[
        \lspan{H} = \set{\lincomb{a}{x}{p} : p \in \N, \spc \dlst{x}{p} \in L, \spc \ulst{a}{p} \in \K}
    \]
\end{definition}

En las siguientes proposiciones veremos que $\lspan{H}$ efectivamente contiene a $H$ y que efectivamente es un subespacio vectorial.

\begin{proposition}
    $H \subseteq \lspan{H}$
\end{proposition}

\begin{proof}
    Si $\vx \in H \implies \underbrace{1}_{\in \K} \cdot \underbrace{\vx}_{\in H} = \vx \in \lspan{H} \implies H \subseteq \lspan{H}$
    $ $.
\end{proof}

\begin{proposition}
    $\lspan{H}$ es un subespacio vectorial de $V$.
\end{proposition}

\begin{proof}
    Veamos que $\lspan{H}$ es parte estable.

    Sean $\vx, \vy \in \lspan{H} \Rightarrow \exists p, q \in \N, \spc \dlst{\vx}{p}, \dlst{\vy}{q} \in H, \, \, \ulst{a}{p}, \ulst{b}{q} \in \K $
    tal que 
    \begin{align*}
        \vx &= \lincomb{a}{x}{p} \\
        \vy &= \lincomb{b}{y}{q}
    \end{align*}

    Entonces podemos representar $\vx + \vy$ como
    
    \[
        \vx + \vy = a^1\underbrace{\vx_1}_{\in H}+a^2\underbrace{\vx_2}_{\in H}+\dots+a^p\underbrace{\vx_p}_{\in H} + b^1\underbrace{\vy_1}_{\in H}+b^2\underbrace{\vy_2}_{\in H}+\dots+b^p\underbrace{\vy_p}_{\in H}
    \]

    Por tanto $\vx + \vy$ es una combinación lineal de vectores de $H$, por lo que $\vx + \vy \in \lspan{H}$. Analizemos ahora el producto por escalar. Sea $a \in \K, \vx \in \lspan{H} \implies \exists p \in \N, \space \exists \dlst{\vx}{p} \in H$ y también $\exists \ulst{a}{p} \in \K \st \vx = \lincomb{a}{x}{p}$.  Multiplicamos a ambos lados por $a$:
    \[a\cdot \vx = a \left( \lincomb{a}{x}{p} \right) = \underbrace{(a \cdot a^1)}_{\in \K}\underbrace{\vx_1}_{\in H} + \underbrace{(a \cdot a^2)}_{\in \K}\underbrace{\vx_2}_{\in H} + \dots + \underbrace{(a \cdot a^p)}_{\in \K}\underbrace{\vx_p}_{\in H}\]

    Por tanto $a\vx$ es una combinación lineal finita de vectores de $H$, por lo que $a\vx \in \lspan{H}$.

    Concluimos por tanto que $\lspan{H}$ es un subespacio vectorial.
\end{proof}

De hecho, $\lspan{H}$ no es solamente un subespacio vectorial que contiene a $H$, sino que es el subespacio vectorial más pequeño que contiene a $H$. 

\begin{proposition}
    Si $L$ es un subespacio vectorial de $V$ tal que $H \subseteq L \implies \lspan{H} \subseteq L$, es decir, $\lspan{H}$ es el subespacio más pequeño de los subespacios vectoriales que contienen a $H$.¡
\end{proposition}

\begin{proof}
Sea $\vx \in \lspan{H} \Rightarrow \exists p \in \N, \spc \dlst{x}{p} \in H, \spc \ulst{a}{p} \in \K$ tal que 

\[ \vx = a^1 \underbrace{\vx_1}_{\in H \subseteq L} +  a^2\underbrace{\vx_2}_{\in H \subseteq L} + \dots +  a^p \underbrace{\vx_p}_{\in H \subseteq L} \]

Como cada $\vx_i \in H \subseteq L$, entonces cada $\vx_i \in L$, por lo que $x$ es combinación lineal de vectores de $L$ y por ser $L$ un espacio vectorial, $\vx \in L$. Como esto se da para cada $\vx \in \lspan{H}$, $\lspan{H} \subseteq L$.
\end{proof}

\subsection{Sistemas generadores y linealmente independientes}

Hasta ahora hemos utilizado $\lspan{H}$ para encontrar un subespacio vectorial conociendo $H$. Sin embargo, podemos utilizarlo a la inversa. Dado un espacio vectorial $V$, si conseguimos expresarlo como $V=\lspan{H}$ para algún $H$ pequeño (por ejemplo, finito), sabemos que podemos expresar todos los vectores de $V$ como combinaciones lineales de vectores de $H$, lo cuál simplifica muchas cuentas.

Introduzcamos algunas definiciones basadas en esta idea.

\begin{definition}[Subespacio generado]
    Diremos que $\lspan{H}$ es el subespacio vectorial generado por $H$.
\end{definition}

\begin{definition}[Sistema de generadores]
    Diremos que $H$ es un sistema de generadores de $V$ si $\lspan{H} = V$.
\end{definition}

\begin{definition}[Finitamente generado]
    Diremos que $V$ es finitamente generado o de dimensión finita si existe un sistema de generadores de $V$ formado por un número finitio de elementos, es decir, si
    \[
        \exists p \in N, \spc \exists \dlst{x}{p} \in V \st \lspan{\set{\vlst{x}{p}}} = V
    \]  
\end{definition}

Ya hemos comentado anteriormente que cuanto más \textit{pequeño} sea $H$, más convenientes resultarán los cálculos, ya que podremos descomponer cualquier vector en una combinación lineal de menores vectores. Si uno de los vectores de $H$ fuese combinación lineal de los otros vectores, entonces no aportaría nada a $\lspan{H}$, podríamos eliminarlo y $\lspan{H'}$ seguiría manteniendose igual, y habríamos conseguido eliminar un vector de $H$. 

De este concepto nace la siguiente definición:

\begin{definition}[Familia linealmente dependiente]
    Si $p \ge 2$ y $\vlst{x}{p} \in V$. Diremos que la familia $\set{\vlst{x}{p}}$ es linealmente dependiente si uno de ellos es combinación lineal de los otros. En el caso $p=1$, un único vector es linealmente dependiente solo si es el vector nulo, en caso contrario, es linealmente independiente. Alternativamente, la familia $\set{\vlst{x}{p}}$ es linealmente dependiente si
    \[\exists i \in \set{1, 2, \dots, p} \st \vx_i \in \lspan{\set{\vlst{x}{i-1}, \vx_{i+1}, \dots, \vx_p}}
    \]
\end{definition}

Muchas veces nos resultará más útil hablar de \textit{familias linealmente independientes}.

\begin{definition}[Familia linealmente independiente]
    La familia $\set{\vlst{x}{p}}$ es linealmente independiente si no es linealmente dependiente. Alternativamente, $\set{\vlst{x}{p}}$ es linealmente independiente si
    \[\forall i \in \set{1, 2, \dots, p}, \spc \vx_i \notin \lspan{\set{\vlst{x}{i-1}, \vx_{i+1}, \dots, \vx_p}}
    \]
\end{definition}

Existe otra caracterización muy importante de la dependencia lineal, que nos permitirá más adelante demostrar resultados importantes.

\begin{proposition}
   Sean $\set{\vlst{x}{p}} \in V$. Entonces  $\set{\vlst{x}{p}}$ son linealmente dependientes si y solo si $\exists \slst{a}{p} \in \K$ no todos nulos tal que $\lincomb{a}{x}{p} = \zv$

\end{proposition}

\begin{proof}

    Analicemos las dos implicaciones por separado.
    \begin{enumerate}
        \item[\protect\fbox{$\Rightarrow$}] Supongamos $\set{\vlst{x}{p}}$ linealmente dependientes, es decir 
        \[\exists i \in \set{1, 2, \dots, p} \st \vx_i \in \lspan{\set{\vlst{x}{i-1}, \vx_{i+1}, \dots, \vx_p}}\]
        Por tanto, $\exists \slst{a}{i-1}, a^{i+1}, a^p \in \K$ tales que 
        \[
            \vx_i = \lincomb{a}{x}{i-1}+a^{i+1}\vx_{i+1}+\dots+a^p \vx_p  
        \]
        Restando $\vx_i$ a ambos lados de la ecuación llegamos a que
        \[
            \vx_i - \vx_i = \zv  = \lincomb{a}{x}{i-1}+\underbrace{(-1)}_{\ne 0}\vx_i +a^{i+1}\vx_{i+1}+\dots+a^p \vx_p
        \]
        \item[\protect\fbox{$\Leftarrow$}] Supongamos que $\exists \slst{a}{p} \in \K$ no todos nulos tal que $\lincomb{a}{x}{p} = 0$. Sea $i\in \set{1, \dots, p} \st a^i \ne 0 \Rightarrow \frac{1}{a} \in \K$. Por tanto,
        \begin{align*}
            \zv &= \frac{1}{a^i} \cdot \zv = \frac{1}{a^i}\parens{ \lincomb{a}{x}{i-1} + a^i\vx_i + a^{i+1}\vx_{i+1} + \dots a^p\vx_p} \\
                &= \parens{\frac{a^1}{a^i}}\vx_1 + \parens{\frac{a^2}{a^i}}\vx_2 + \dots + \parens{\frac{a^{i-1}}{a^i}}\vx_{i-1} + \underbrace{\parens{\frac{a^{i}}{a^i}}}_{1}\vx_{i} + \parens{\frac{a^{i+1}}{a^i}}\vx_{i+1} + \dots + \parens{\frac{a^{p}}{a^i}}\vx_{p}
        \end{align*}
        El coeficiente de $\vx_{i-1}$ es $1$, así que basta pasar el resto de términos que le acompañan restando para despejar $\vx_{i-1}$, obteniendo 
        \[
           \vx_{i} = \underbrace{\parens{\frac{-a^1}{a^i}}}_{\in \K}\vx_1 + \underbrace{\parens{\frac{-a^2}{a^i}}}_{\in \K}\vx_2 + \dots + \underbrace{\parens{\frac{-a^{i-1}}{a^i}}}_{\in \K}\vx_{i-1} + \underbrace{\parens{\frac{-a^{i+1}}{a^i}}}_{\in \K}\vx_{i+1} + \dots + \underbrace{\parens{\frac{-a^{p}}{a^i}}}_{\in \K}\vx_{p}
        \]

        Por tanto, $\vx_i$ es combinación lineal del resto de vectores.
    \end{enumerate}
\end{proof}

Negando los dos lados del si y solo si en la proposición anterior, conseguimos el siguiente corolario.

\begin{corollary}
    \label{linear_independence_corollary}
    Sean $\set{\vlst{x}{p}} \subseteq V$. Entonces $\set{\vlst{x}{p}}$ son linealmente independientes $\iff$ si  $\slst{a}{p} \in \K \st \lincomb{a}{x}{p} = \zv$ entonces $a^i = 0 \spc \forall i \in  \set{1, \dots, p} $. 
\end{corollary}

Una propiedad muy importante de las familias linealmente independientes, es que cualquier vector que se pueda expresar como combinación lineal de ellas se hace de manera única, es decir, no existen dos combinaciones lineales distintas de elementos de la familia. La demostración se apoya en la caracterízación que acabamos de demostrar.

\begin{proposition}
    Sean $\set{\vlst{x}{p}} \subseteq V$. Entonces son equivalentes:
    \begin{enumerate}[a)]
        \item $\set{\vlst{x}{p}}$ son linealmente independientes.
        \item Cualquier vector de $\lspan{\set{\vlst{x}{p}}}$ se puede expresar como una única combinación lineal.  Es decir, $\forall \vx \in \lspan{\set{\vlst{x}{p}}}, \spc \exists! \, \slst{a}{p} \in \K$ tal que $\vx = \lincomb{a}{x}{p}$. 
    \end{enumerate}
\end{proposition}

\begin{proof}

    Analicemos las dos implicaciones por separado.
    \begin{enumerate}[itemindent=20pt]
        \item[\protect\fbox{a) $\Rightarrow$ b)}] Supongamos que $\set{\vlst{x}{p}}$ son l.i. (linealmente independientes). \\
        Sea $\vx \in \lspan{\set{\vlst{x}{p}}} \Rightarrow \exists \slst{a}{p} \in \K \st \vx = \lincomb{a}{x}{p}$. Sean $\slst{a}{p} \in \K \st \vx = \lincomb{a}{x}{p}$. Entonces, restando:
        \begin{align*}
            \zv &= \vx - \vx = \lincomb{a}{x}{p} - (\lincomb{b}{x}{p}) = \\
                &= \parens{a^1-b^1}\vx_1 + \parens{a^2-b^2}\vx_2 + \dots + \parens{a^p-b^p}\vx_p
        \end{align*}
        Como hemos supuestos que $\set{\vlst{x}{p}}$ son l.i., por el corolario \ref{linear_independence_corollary} todos los coeficientes son 0, es decir,
        \[
            \begin{array}[t].{l}\}
            a^1-b^1 = 0 \\
            a^2-b^2 = 0 \\ 
            \hspace{12.74mm}\vdots \\
            a^p-b^p = 0
            \end{array}
            \begin{array}[t]{@{}c@{}}\\{}\implies {}\\{}\end{array}
            \begin{array}[t].{l}\}
                a^1 = b^1 \\
                a^2 = b^2 \\ 
                \hspace{5.3mm}\vdots \\
            a^p = b^p
            \end{array}
        \]
    
        \item[\protect\fbox{b) $\Rightarrow$ a)}] Supongamos que si $\vx \in \lspan{\set{\vlst{x}{p}}}, \spc \exists ! \, \slst{a}{p} \in \K $ tal que 
        \(\vx = \lincomb{a}{x}{p}\). Demostremos que $\set{\vlst{x}{p}}$ son linealmente independientes utilizando la caracterización. Sean $\slst{b}{p}\in \K$ tal que $\lincomb{b}{x}{p} = \zv$. Además, por ser $0\cdot \vx = \zv \spc \forall x \in V$, tenemos que $0 \cdot \vx_1 + 0 \cdot \vx_2 + \dots + 0 \cdot \vx_p = \zv$. El vector nulo se puede expresar entonces como dos combinaciones lineales:

        \[
        \begin{cases}
            \lincomb{b}{x}{p} &= \zv \\
            0 \cdot \vx_1 + 0 \cdot \vx_2 + \dots + 0 \cdot \vx_p &= \zv
        \end{cases}
        \]
        Como solamente existe una única combinación lineal que represente a $\zv$, entonces $b^i = 0 \spc \forall i \in \set{1, \dots, p} $ 
    \end{enumerate}
\end{proof}

    La siguiente proposición nos ayudará a expandir una familia de vectores que ya sabemos que son linealmente independientes con un nuevo vector bajo ciertas condiciones.

    \begin{proposition}
        Sean $\set{\vlst{x}{p}}$ linealmente independientes y sea $\vx \notin \lspan{\set{\vlst{x}{p}}}$, es decir, $\vx$ no es combinación lineal de $\set{\vlst{x}{p}}$. Entonces $\set{\vx, \vlst{x}{p}}$ son l.i.
    \end{proposition}

    \begin{proof}
        Sean $a,\slst{a}{p} \in \K$ tal que $a\vx + \lincomb{a}{x}{p} = 0$
        Veamos por reducción al absurdo que neecsariamente $a = 0$.
        \begin{enumerate}[itemindent=20pt]
            \item[\protect\fbox{si $a \ne 0$}] 
            \[
                \zv = \frac{1}{a} \cdot \zv = \frac{1}{a} \left( a\vx + a^1 \vx_1 + \dots + a^p\vx_p  \right) = \vx + \frac{a^1}{a}\vx_1 + \dots + \frac{a^p}{a}\vx_p
            \]
            Despejando $\vx$ restando
            \[
                \vx = \parens{\frac{-a^1}{a}}\vx_1 + \dots + \parens{\frac{-a^p}{a}}\vx_p
            \]
            Hemos llegado a una contradicción porque hemos podido expresar $\vx$ como combinación lineal de $\set{\vlst{x}{p}}$, por tanto, necesariamente $a=0$
            \item[\protect\fbox{si $a = 0$}] Como $a=0$, solamente necesitamos ver que $a_i=0 \spc \forall i \in \set{1, \dots, p}$.
            \[
                \zv = \underbrace{0 \cdot \vx}_{\zv} + \lincomb{a}{x}{p} = \lincomb{a}{x}{p}
            \] 

            Como $\set{\vlst{x}{p}}$ son l.i., $a_i=0 \spc \forall i \in \set{1, \dots, p}$. Como además $a=0$, tenemos que $\set{x, \vlst{x}{p}}$ son l.i.
        \end{enumerate}
    \end{proof}

    \begin{remark}
        A pesar de ser un error relativamente común, si existen 4 vectores linealmente independientes 3 a 3 entre sí, eso no implica que los 4 vectores sean linealmente independientes. Un contraejemplo de esto son los vectores $\set{(1,0,0),(0,1,0),(0,0,1),(1,1,1)}$ de $\R^3$.
    \end{remark}

\section{Teoremas de la base}

\end{document}
